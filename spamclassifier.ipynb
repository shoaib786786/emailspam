{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Necessary Libraries\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from flask import Flask, request, jsonify\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from collections.abc import Sequence\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/mohammedzaidsyed/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/mohammedzaidsyed/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create folders if they do not exist\n",
    "if not os.path.exists('models'):\n",
    "    os.makedirs('models')\n",
    "if not os.path.exists('mlruns'):\n",
    "    os.makedirs('mlruns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tracking URI: 'file:///Users/mohammedzaidsyed/Desktop/Spam/spamclassification/mlruns'\n",
      "3\n",
      "Experiment Name: SVM, Experiment ID: 407553083182888926\n",
      "Experiment Name: Random Forest, Experiment ID: 266853881450216497\n",
      "Experiment Name: Naive Bayes, Experiment ID: 831114447560693852\n"
     ]
    }
   ],
   "source": [
    "print(f\"tracking URI: '{mlflow.get_tracking_uri()}'\")\n",
    "\n",
    "client = MlflowClient()\n",
    "\n",
    "# Use the search_experiments function to get a list of available experiments\n",
    "experiments = client.search_experiments()\n",
    "print(len(experiments))\n",
    "\n",
    "# Display the list of available experiments\n",
    "for experiment in experiments:\n",
    "    print(f\"Experiment Name: {experiment.name}, Experiment ID: {experiment.experiment_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set MLflow tracking URI to log to the desired folder\n",
    "mlflow.set_tracking_uri(\"file:///Users/mohammedzaidsyed/Desktop/Spam/spamclassification/mlruns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "def load_data(path):\n",
    "    return pd.read_csv(path)\n",
    "\n",
    "df = load_data('data/spam_ham_dataset.csv')\n",
    "df.head()\n",
    "X = df['text'].tolist()\n",
    "y = df['label'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(word) for word in tokens]\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the dataset\n",
    "X_preprocessed = [preprocess_text(text) for text in X]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the preprocessed data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_preprocessed, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORICAL_COLS = [\"label\", \"text\", \"label_num\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import pickle\n",
    "import os\n",
    "import mlflow\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "def encode_cols(df: pd.DataFrame, categorical_cols: List[str] = None) -> pd.DataFrame:\n",
    "    if categorical_cols is None:\n",
    "        categorical_cols = [\"label\", \"text\", \"label_num\"]\n",
    "    \n",
    "    # Iterate over each column name in the list\n",
    "    for col in categorical_cols:\n",
    "        df[col] = df[col].astype(str).str.lower()\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def extract_x_y(\n",
    "    df: pd.DataFrame,\n",
    "    categorical_cols: List[str] = None,\n",
    "    numerical_cols: List[str] = None,\n",
    "    dv: DictVectorizer = None,\n",
    "    with_target: bool = True,\n",
    ") -> dict:\n",
    "    if categorical_cols is None:\n",
    "        categorical_cols = [\"label\", \"text\", \"label_num\"]\n",
    "    \n",
    "    dicts = df[[*categorical_cols]].to_dict(orient=\"records\")\n",
    "\n",
    "    y = None\n",
    "    if with_target:\n",
    "        if dv is None:\n",
    "            dv = DictVectorizer()\n",
    "            dv.fit(dicts)\n",
    "        y = df[\"label\"].values\n",
    "\n",
    "    x = dv.transform(dicts)\n",
    "    return x, y, dv\n",
    "\n",
    "def save_pickle(path: str, file):\n",
    "    with open(path, \"wb\") as f:\n",
    "        pickle.dump(file, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models\n",
    "models = {\n",
    "    \"Naive Bayes\": MultinomialNB(),\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"SVM\": SVC()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'columns'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/mohammedzaidsyed/Desktop/Spam/spamclassification/spamclassifier.ipynb Cell 15\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mohammedzaidsyed/Desktop/Spam/spamclassification/spamclassifier.ipynb#X15sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m categorical_cols \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mlabel_num\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m# Fill in the actual names of your categorical columns\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mohammedzaidsyed/Desktop/Spam/spamclassification/spamclassifier.ipynb#X15sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m# Encode categorical columns\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/mohammedzaidsyed/Desktop/Spam/spamclassification/spamclassifier.ipynb#X15sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m df_train_encoded \u001b[39m=\u001b[39m encode_cols(X_train, categorical_cols)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mohammedzaidsyed/Desktop/Spam/spamclassification/spamclassifier.ipynb#X15sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39m# Extract features and target variable\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mohammedzaidsyed/Desktop/Spam/spamclassification/spamclassifier.ipynb#X15sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m X_train_transformed, y_train, dv \u001b[39m=\u001b[39m extract_x_y(df_train_encoded)\n",
      "\u001b[1;32m/Users/mohammedzaidsyed/Desktop/Spam/spamclassification/spamclassifier.ipynb Cell 15\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mohammedzaidsyed/Desktop/Spam/spamclassification/spamclassifier.ipynb#X15sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m# Iterate over each column name in the list\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mohammedzaidsyed/Desktop/Spam/spamclassification/spamclassifier.ipynb#X15sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mfor\u001b[39;00m col \u001b[39min\u001b[39;00m categorical_cols:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mohammedzaidsyed/Desktop/Spam/spamclassification/spamclassifier.ipynb#X15sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     \u001b[39m# Apply encoding operations only if the column exists in the DataFrame\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/mohammedzaidsyed/Desktop/Spam/spamclassification/spamclassifier.ipynb#X15sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     \u001b[39mif\u001b[39;00m col \u001b[39min\u001b[39;00m df_encoded\u001b[39m.\u001b[39mcolumns:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mohammedzaidsyed/Desktop/Spam/spamclassification/spamclassifier.ipynb#X15sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m         df_encoded[col] \u001b[39m=\u001b[39m df_encoded[col]\u001b[39m.\u001b[39mastype(\u001b[39mstr\u001b[39m)\u001b[39m.\u001b[39mstr\u001b[39m.\u001b[39mlower()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mohammedzaidsyed/Desktop/Spam/spamclassification/spamclassifier.ipynb#X15sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mreturn\u001b[39;00m df_encoded\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'columns'"
     ]
    }
   ],
   "source": [
    "# Loop over models and track experiments\n",
    "for name, model in models.items():\n",
    "    # Start a new MLflow experiment\n",
    "    mlflow.set_experiment(name)\n",
    "    \n",
    "    with mlflow.start_run():\n",
    "        # Assuming you have defined X_train, X_test, y_train, y_test properly\n",
    "        \n",
    "        # Define your list of categorical columns to encode\n",
    "        categorical_cols = [\"label\", \"text\", \"label_num\"] # Fill in the actual names of your categorical columns\n",
    "        \n",
    "        # Encode categorical columns\n",
    "        df_train_encoded = encode_cols(X_train, categorical_cols)\n",
    "        \n",
    "        # Extract features and target variable\n",
    "        X_train_transformed, y_train, dv = extract_x_y(df_train_encoded)\n",
    "        \n",
    "        # Save preprocessor\n",
    "        save_pickle(\"/Users/mohammedzaidsyed/Desktop/Spam/spamclassification/web_service/saved_pkl/dv__v0.0.1.pkl\", dv)\n",
    "        \n",
    "        # Initialize CountVectorizer\n",
    "        vectorizer = CountVectorizer()\n",
    "\n",
    "        # Fit and transform the training data\n",
    "        X_train_counts = vectorizer.fit_transform(X_train_transformed)\n",
    "        \n",
    "        # Train the model\n",
    "        model.fit(X_train_counts, y_train)\n",
    "        \n",
    "        # Transform the test data\n",
    "        X_test_counts = vectorizer.transform(X_test)\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred = model.predict(X_test_counts)\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        \n",
    "        # Log model parameters and metrics\n",
    "        mlflow.log_param(\"Model\", name)\n",
    "        mlflow.log_metric(\"Accuracy\", accuracy)\n",
    "\n",
    "        # Save the model\n",
    "        model_path = f\"/Users/mohammedzaidsyed/Desktop/Spam/spamclassification/web_service/models/{name}\"  # Adjust the path\n",
    "        os.makedirs(model_path, exist_ok=True)\n",
    "        mlflow.sklearn.save_model(model, model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the models from the specified folder\n",
    "model_nb = mlflow.sklearn.load_model(\"../models/Naive Bayes\")\n",
    "model_rf = mlflow.sklearn.load_model(\"../models/Random Forest\")\n",
    "model_svm = mlflow.sklearn.load_model(\"../models/SVM\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Flask app\n",
    "app = Flask(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction endpoint\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    data = request.get_json(force=True)\n",
    "    email_text = data['email_text']\n",
    "\n",
    "    # Make predictions with each model\n",
    "    pred_nb = model_nb.predict([preprocess_text(email_text)])[0]\n",
    "    pred_rf = model_rf.predict([preprocess_text(email_text)])[0]\n",
    "    pred_svm = model_svm.predict([preprocess_text(email_text)])[0]\n",
    "\n",
    "    # Combine predictions\n",
    "    predictions = {\n",
    "        \"Naive Bayes\": bool(pred_nb),\n",
    "        \"Random Forest\": bool(pred_rf),\n",
    "        \"SVM\": bool(pred_svm)\n",
    "    }\n",
    "\n",
    "    return jsonify(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n",
      " * Restarting with watchdog (fsevents)\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/mohammedzaidsyed/anaconda3/lib/python3.11/site-packages/ipykernel_launcher.py\", line 15, in <module>\n",
      "    from ipykernel import kernelapp as app\n",
      "  File \"/Users/mohammedzaidsyed/anaconda3/lib/python3.11/site-packages/ipykernel/__init__.py\", line 5, in <module>\n",
      "    from .connect import *  # noqa\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mohammedzaidsyed/anaconda3/lib/python3.11/site-packages/ipykernel/connect.py\", line 11, in <module>\n",
      "    import jupyter_client\n",
      "  File \"/Users/mohammedzaidsyed/anaconda3/lib/python3.11/site-packages/jupyter_client/__init__.py\", line 8, in <module>\n",
      "    from .asynchronous import AsyncKernelClient  # noqa\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mohammedzaidsyed/anaconda3/lib/python3.11/site-packages/jupyter_client/asynchronous/__init__.py\", line 1, in <module>\n",
      "    from .client import AsyncKernelClient  # noqa\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mohammedzaidsyed/anaconda3/lib/python3.11/site-packages/jupyter_client/asynchronous/client.py\", line 8, in <module>\n",
      "    from jupyter_client.client import KernelClient\n",
      "  File \"/Users/mohammedzaidsyed/anaconda3/lib/python3.11/site-packages/jupyter_client/client.py\", line 22, in <module>\n",
      "    from .connect import ConnectionFileMixin\n",
      "  File \"/Users/mohammedzaidsyed/anaconda3/lib/python3.11/site-packages/jupyter_client/connect.py\", line 27, in <module>\n",
      "    from jupyter_core.paths import jupyter_data_dir\n",
      "  File \"/Users/mohammedzaidsyed/anaconda3/lib/python3.11/site-packages/jupyter_core/paths.py\", line 19, in <module>\n",
      "    from pathlib import Path\n",
      "  File \"/Users/mohammedzaidsyed/anaconda3/lib/python3.11/site-packages/pathlib.py\", line 10, in <module>\n",
      "    from collections import Sequence\n",
      "ImportError: cannot import name 'Sequence' from 'collections' (/Users/mohammedzaidsyed/anaconda3/lib/python3.11/collections/__init__.py)\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mohammedzaidsyed/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3534: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    app.run(port=5000, debug=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
